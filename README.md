# MasterThesis_CausalQL

This research thesis is dedicated to explore the integration of causal models into Reinforcement Learning (RL) algorithms, a concept known as Causal Reinforcement Learning. This approach holds significant promise across several domains such as healthcare, robotics, environment and economics. By combining data-driven learning with causal reasoning, Causal RL has the potential to create adaptable, robust, and trustworthy learning systems capable of excelling in complex and uncertain real-world scenarios.

Specifically, this thesis focuses on integrating causal knowledge into the Q-Learning algorithm, a form of Temporal-Difference learning algorithm widely used in RL. While Q-Learning may not be well-suited for dynamic environments due to its construction, introducing causal knowledge has shown the ability to enhance its performance. The causal knowledge is extracted through causal inference, employing operations like do-calculus to deduce the posterior probabilities of variables after specific actions. The primary objective of this work is to provide a comprehensive framework that seamlessly integrates causal knowledge into the Q-Learning algorithm. Additionally, the concepts of explainability and trustworthiness have been taken into account to guarantee the interpretability and transparent comprehension of the agent's decision-making process.

The approach outlined in this thesis consists of two primary components. The initial phase entails the agent's navigation within a compact environment featuring a solitary randomly moving enemy. During this phase, the agent leverages causal inference to understand the outcomes resulting from its actions, and these outcomes are stored for subsequent use. In the latter phase, the agent operates within larger environments of diverse sizes and configurations of enemies, with the overarching objective of reaching designated goal positions.

The evaluation of the proposed approach is facilitated through the creation of a game environment. The game involves an agent navigating an environment to reach a goal position, without having any prior knowledge of the environment's layout. An in-depth comparison is conducted between the performance of the classic Q-Learning algorithm and the novel Causal Q-Learning algorithms; it is achieved through an extensive experimental campaign encompassing various scenarios, ranging from easy to challenging. The comparison of results, focuses on several key metrics, including the average reward per episode step, the number of steps necessary to finish an episode, the average time taken to complete a game using each designed approach, the frequency of defeats for each algorithm, and the occurrences of timeout conditions for each algorithm. The experimental results highlight the significant performance advantage of agents equipped with causal knowledge compared to those using only classic Q-Learning. Notably, the performance gap widens as task complexity increases.

The importance of this endeavor rests in the advancement of Causal Reinforcement Learning and Causal Artificial Intelligence, which adds to the progress of scientific research by introducing a novel class of AI algorithms. These algorithms are able to understand and evaluate complex environments, all while maintaining the capacity to continuously acquire new concepts. The learning and comprehension mechanisms of these Causal Reinforcement Learning algorithms closely mirror human cognitive processes.
